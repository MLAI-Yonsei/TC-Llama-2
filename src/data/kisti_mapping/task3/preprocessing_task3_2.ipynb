{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b516041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "670dd559",
   "metadata": {},
   "outputs": [],
   "source": [
    "lv1_key2idx = json.load(open(\"./lv1_key2idx.json\"))\n",
    "lv2_key2idx = json.load(open(\"./lv2_key2idx.json\"))\n",
    "lv3_key2idx = json.load(open(\"./lv3_key2idx.json\"))\n",
    "lv4_key2idx = json.load(open(\"./lv4_key2idx.json\"))\n",
    "lv1_label_idx = json.load(open(\"./lv1_label_idx.json\"))\n",
    "unscp_lv1 = json.load(open(\"./new_unscp_lv1.json\"))\n",
    "unscp_lv1idx2lv1idx = json.load(open(\"./unscp_lv1idx2lv1idx.json\"))\n",
    "\n",
    "lv1idx2lv5idx = json.load(open(\"./lv1idx2lv5idx.json\"))\n",
    "lv5_label_idx = json.load(open(\"./lv5_label_idx.json\"))\n",
    "lv5_idx2lv1_idx = json.load(open(\"./lv5_idx2lv1_idx.json\"))\n",
    "lv5_idx2lv2_idx = json.load(open(\"lv5_idx2lv2_idx.json\"))\n",
    "lv5_idx2lv3_idx = json.load(open(\"lv5_idx2lv3_idx.json\"))\n",
    "lv5_idx2lv4_idx = json.load(open(\"lv5_idx2lv4_idx.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "172af003",
   "metadata": {},
   "outputs": [],
   "source": [
    "lv1_idx2key = {}\n",
    "for k, v in lv1_key2idx.items():\n",
    "    lv1_idx2key[v]=k\n",
    "    \n",
    "lv2_idx2key = {}\n",
    "for k, v in lv2_key2idx.items():\n",
    "    lv2_idx2key[v]=k\n",
    "    \n",
    "lv3_idx2key = {}\n",
    "for k, v in lv3_key2idx.items():\n",
    "    lv3_idx2key[v]=k\n",
    "    \n",
    "lv4_idx2key = {}\n",
    "for k, v in lv4_key2idx.items():\n",
    "    lv4_idx2key[v]=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8f4f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_emb_2048_eos = torch.load(\"../../../kisti_output/emb_zodal_cat_max_leng_2048_eos.pt\", map_location='cpu')\n",
    "cat_emb_512_eos = torch.load(\"../../../kisti_output/emb_zodal_cat_max_leng_512_eos.pt\", map_location='cpu')\n",
    "cat_emb_2048_masked = torch.load(\"../../../kisti_output/emb_zodal_cat_max_leng_2048_masked.pt\", map_location='cpu')\n",
    "cat_emb_512_masked = torch.load(\"../../../kisti_output/emb_zodal_cat_max_leng_512_masked.pt\", map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cadaca83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21829, 21829, 21829, 21829)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_emb_2048_eos), len(cat_emb_2048_masked), len(cat_emb_512_eos), len(cat_emb_512_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "525b2945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21829, 21829, 21829)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lv5_idx2lv2_idx), len(lv5_idx2lv3_idx), len(lv5_idx2lv4_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26951fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 레벨 category 각각에 속하는 Lv5 categories 를 list type으로 수집\n",
    "def higher2lv5list(lv5idx2idx):\n",
    "    _tmp = defaultdict(list)\n",
    "    for k, v in lv5idx2idx.items():\n",
    "        _tmp[v].append(int(k))\n",
    "    _tmp = dict(sorted(_tmp.items(), key=lambda x: x[0]))\n",
    "    return _tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd4ad940",
   "metadata": {},
   "outputs": [],
   "source": [
    "lv1_idx2lv5_idx_list = higher2lv5list(lv5_idx2lv1_idx)\n",
    "lv2_idx2lv5_idx_list = higher2lv5list(lv5_idx2lv2_idx)\n",
    "lv3_idx2lv5_idx_list = higher2lv5list(lv5_idx2lv3_idx)\n",
    "lv4_idx2lv5_idx_list = higher2lv5list(lv5_idx2lv4_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166622c1-c037-4dd2-bb26-ce87de49b477",
   "metadata": {},
   "source": [
    "### Lv 5 Embedding 을 상위 (Lv 1) 카테고리별로 군집한 후 평균 계산\n",
    "---\n",
    "- 상위 레벨 (Lv1)에 공통으로 속하는 Lv 5 embedding 을 평균낸 임베딩을 해당 상위 레벨을 표현하는 임베딩으로 사용.\n",
    "- 상위 레벨 (Lv1)에 공통으로 속하는 Lv 5 embedding 을 tensor 로 묶어서 따로 저장."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69e9a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_emb_lv1_2048_eos = []\n",
    "cat_emb_lv1_2048_masked = []\n",
    "cat_emb_lv1_512_eos = []\n",
    "cat_emb_lv1_512_masked = []\n",
    "for k, vs in lv1_idx2lv5_idx_list.items():\n",
    "    idx_tensor = torch.tensor(vs)\n",
    "    avg_emb_2048_eos = cat_emb_2048_eos[idx_tensor].mean(dim=0).tolist()\n",
    "    avg_emb_2048_masked = cat_emb_2048_masked[idx_tensor].mean(dim=0).tolist()\n",
    "    avg_emb_512_eos = cat_emb_512_eos[idx_tensor].mean(dim=0).tolist()\n",
    "    avg_emb_512_masked = cat_emb_512_masked[idx_tensor].mean(dim=0).tolist()\n",
    "    \n",
    "    cat_emb_lv1_2048_eos.append(avg_emb_2048_eos)\n",
    "    cat_emb_lv1_2048_masked.append(avg_emb_2048_masked)\n",
    "    cat_emb_lv1_512_eos.append(avg_emb_512_eos)\n",
    "    cat_emb_lv1_512_masked.append(avg_emb_512_masked)\n",
    "\n",
    "cat_emb_lv1_2048_eos = torch.tensor(cat_emb_lv1_2048_eos).to(dtype=torch.float16)\n",
    "cat_emb_lv1_2048_masked = torch.tensor(cat_emb_lv1_2048_masked).to(dtype=torch.float16)\n",
    "cat_emb_lv1_512_eos = torch.tensor(cat_emb_lv1_512_eos).to(dtype=torch.float16)\n",
    "cat_emb_lv1_512_masked = torch.tensor(cat_emb_lv1_512_masked).to(dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "185d453c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 54, 54, 54, 54)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lv1_idx2key.keys()), len(cat_emb_lv1_2048_eos), len(cat_emb_lv1_2048_masked), len(cat_emb_lv1_512_eos), len(cat_emb_lv1_512_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "667be858",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_unspsc_lv1_max_leng_512_eos = torch.load(\"../../../kisti_output/emb_unspsc_lv1_max_leng_512_eos.pt\", map_location='cpu')\n",
    "emb_unspsc_lv1_max_leng_512_masked = torch.load(\"../../../kisti_output/emb_unspsc_lv1_max_leng_512_masked.pt\", map_location='cpu')\n",
    "emb_unspsc_lv1_max_leng_2048_eos = torch.load(\"../../../kisti_output/emb_unspsc_lv1_max_leng_2048_eos.pt\", map_location='cpu')\n",
    "emb_unspsc_lv1_max_leng_2048_masked = torch.load(\"../../../kisti_output/emb_unspsc_lv1_max_leng_2048_masked.pt\", map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abd70528",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_zodal_cat_lv1_max_leng_512_eos = torch.load(\"../../../kisti_output/emb_zodal_cat_lv1_max_leng_512_eos.pt\", map_location='cpu')\n",
    "emb_zodal_cat_lv1_max_leng_512_masked = torch.load(\"../../../kisti_output/emb_zodal_cat_lv1_max_leng_512_masked.pt\", map_location='cpu')\n",
    "emb_zodal_cat_lv1_max_leng_2048_eos = torch.load(\"../../../kisti_output/emb_zodal_cat_lv1_max_leng_2048_eos.pt\", map_location='cpu')\n",
    "emb_zodal_cat_lv1_max_leng_2048_masked = torch.load(\"../../../kisti_output/emb_zodal_cat_lv1_max_leng_2048_masked.pt\", map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "972491d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unscp_lv1idx2lv1idx = json.load(open(\"unscp_lv1idx2lv1idx.json\"))\n",
    "zodal_lv1idx2lv1idx = json.load(open(\"zodal_lv1idx2lv1idx.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c8838fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_sim_un(a,b):\n",
    "    a = a.cuda()\n",
    "    b = b.cuda()\n",
    "    a_norm = torch.norm(a, dim=-1)\n",
    "    b_norm = torch.norm(b, dim=-1)\n",
    "\n",
    "        \n",
    "    sim_list = []\n",
    "    for k, v in unscp_lv1idx2lv1idx.items():\n",
    "        sim = torch.matmul(a[int(k)], b[v].T)/(a_norm[int(k)]*b_norm[v])\n",
    "        sim = sim.detach().cpu()\n",
    "        sim_list.append(sim)\n",
    "    return sim_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74fcdb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_avg_un = cal_sim_un(emb_unspsc_lv1_max_leng_512_eos, cat_emb_lv1_512_eos)\n",
    "# sim_avg_un = torch.tensor(sim_avg_un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc02e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_sim_zo(a,b):\n",
    "    a = a.cuda()\n",
    "    b = b.cuda()\n",
    "    a_norm = torch.norm(a, dim=-1)\n",
    "    b_norm = torch.norm(b, dim=-1)\n",
    "    \n",
    "    sim_list = []\n",
    "    for k, v in zodal_lv1idx2lv1idx.items():\n",
    "        sim = torch.matmul(a[int(k)], b[v].T)/(a_norm[int(k)]*b_norm[v])\n",
    "        sim = sim.detach().cpu()\n",
    "        sim_list.append(sim)\n",
    "    return sim_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6e5b6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_avg_zo = cal_sim_zo(emb_zodal_cat_lv1_max_leng_512_eos, cat_emb_lv1_512_eos)\n",
    "# sim_avg_zo = torch.tensor(sim_avg_zo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cc84e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_emb_masked_2048 = torch.load(\"../../../kisti_output/emb_task3_pro_max_leng_2048_masked.pt\")\n",
    "pro_emb_masked_512 = torch.load(\"../../../kisti_output/emb_task3_pro_max_leng_512_masked.pt\")\n",
    "pro_emb_eos_2048 = torch.load(\"../../../kisti_output/emb_task3_pro_max_leng_2048_eos.pt\")\n",
    "pro_emb_eos_512 = torch.load(\"../../../kisti_output/emb_task3_pro_max_leng_512_eos.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90909fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pro_cat_sim_mat(a,b):\n",
    "    a = a.cuda()\n",
    "    b = b.cuda()\n",
    "    a_norm = torch.norm(a, dim=-1)\n",
    "    b_norm = torch.norm(b, dim=-1)\n",
    "\n",
    "    sim_mat = []\n",
    "    for i in range(len(a)):\n",
    "        sim_p_pro = []\n",
    "        for j in range(len(b)):\n",
    "            sim = torch.matmul(a[i], b[j].T) / (a_norm[i]*b_norm[j])\n",
    "            sim_p_pro.append(sim.detach().cpu())\n",
    "        sim_mat.append(sim_p_pro)\n",
    "    sim_mat = torch.tensor(sim_mat)\n",
    "    print(sim_mat.mean())\n",
    "    print(sim_mat.std())\n",
    "    print(sim_mat.max())\n",
    "    print(sim_mat.min())\n",
    "    return sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ee4eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_emb_list = [pro_emb_masked_2048, pro_emb_masked_512, pro_emb_eos_2048, pro_emb_eos_512]\n",
    "zod_emb_list = [emb_zodal_cat_lv1_max_leng_2048_masked, emb_zodal_cat_lv1_max_leng_512_masked, emb_zodal_cat_lv1_max_leng_2048_eos, emb_zodal_cat_lv1_max_leng_512_eos]\n",
    "uni_emb_list = [emb_unspsc_lv1_max_leng_2048_masked, emb_unspsc_lv1_max_leng_512_masked, emb_unspsc_lv1_max_leng_2048_eos, emb_unspsc_lv1_max_leng_512_eos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7467f06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_lv1_emb_list = [cat_emb_lv1_2048_masked, cat_emb_lv1_512_masked, cat_emb_lv1_2048_eos, cat_emb_lv1_512_eos]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aad0c4f-2adf-44a0-801a-e05635cf7ad8",
   "metadata": {},
   "source": [
    "### Lv 5 Embedding 을 상위 (Lv 2) 카테고리별로 군집한 후 평균 계산\n",
    "---\n",
    "- 상위 레벨 (Lv2)에 공통으로 속하는 Lv 5 embedding 을 평균낸 임베딩을 해당 상위 레벨을 표현하는 임베딩으로 사용.\n",
    "- 상위 레벨 (Lv2)에 공통으로 속하는 Lv 5 embedding 을 tensor 로 묶어서 따로 저장."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b6faa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_emb_lv2_2048_eos = []\n",
    "cat_emb_lv2_2048_masked = []\n",
    "cat_emb_lv2_512_eos = []\n",
    "cat_emb_lv2_512_masked = []\n",
    "for k, vs in lv2_idx2lv5_idx_list.items():\n",
    "    idx_tensor = torch.tensor(vs)\n",
    "    avg_emb_2048_eos = cat_emb_2048_eos[idx_tensor].mean(dim=0).tolist()\n",
    "    avg_emb_2048_masked = cat_emb_2048_masked[idx_tensor].mean(dim=0).tolist()\n",
    "    avg_emb_512_eos = cat_emb_512_eos[idx_tensor].mean(dim=0).tolist()\n",
    "    avg_emb_512_masked = cat_emb_512_masked[idx_tensor].mean(dim=0).tolist()\n",
    "    \n",
    "    cat_emb_lv2_2048_eos.append(avg_emb_2048_eos)\n",
    "    cat_emb_lv2_2048_masked.append(avg_emb_2048_masked)\n",
    "    cat_emb_lv2_512_eos.append(avg_emb_512_eos)\n",
    "    cat_emb_lv2_512_masked.append(avg_emb_512_masked)\n",
    "\n",
    "cat_emb_lv2_2048_eos = torch.tensor(cat_emb_lv2_2048_eos).to(dtype=torch.float16)\n",
    "cat_emb_lv2_2048_masked = torch.tensor(cat_emb_lv2_2048_masked).to(dtype=torch.float16)\n",
    "cat_emb_lv2_512_eos = torch.tensor(cat_emb_lv2_512_eos).to(dtype=torch.float16)\n",
    "cat_emb_lv2_512_masked = torch.tensor(cat_emb_lv2_512_masked).to(dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a1a089d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(387, 387, 387, 387, 387)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lv2_idx2key.keys()), len(cat_emb_lv2_2048_eos), len(cat_emb_lv2_2048_masked), len(cat_emb_lv2_512_eos), len(cat_emb_lv2_512_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1657155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_lv2_emb_list = [cat_emb_lv2_2048_masked, cat_emb_lv2_512_masked, cat_emb_lv2_2048_eos, cat_emb_lv2_512_eos]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fb2f2e-fcc3-4248-bb5c-4ca942dff5d9",
   "metadata": {},
   "source": [
    "### Lv 5 Embedding 을 상위 (Lv 3) 카테고리별로 군집한 후 평균 계산\n",
    "---\n",
    "- 상위 레벨 (Lv3)에 공통으로 속하는 Lv 5 embedding 을 평균낸 임베딩을 해당 상위 레벨을 표현하는 임베딩으로 사용.\n",
    "- 상위 레벨 (Lv3)에 공통으로 속하는 Lv 5 embedding 을 tensor 로 묶어서 따로 저장."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37f7fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_emb_lv3_2048_eos = []\n",
    "cat_emb_lv3_2048_masked = []\n",
    "cat_emb_lv3_512_eos = []\n",
    "cat_emb_lv3_512_masked = []\n",
    "for i, (k, vs) in enumerate(lv3_idx2key.items()):\n",
    "    try:\n",
    "        idx_tensor = torch.tensor(lv3_idx2lv5_idx_list[i])\n",
    "        avg_emb_2048_eos = cat_emb_2048_eos[idx_tensor].mean(dim=0).tolist()\n",
    "        avg_emb_2048_masked = cat_emb_2048_masked[idx_tensor].mean(dim=0).tolist()\n",
    "        avg_emb_512_eos = cat_emb_512_eos[idx_tensor].mean(dim=0).tolist()\n",
    "        avg_emb_512_masked = cat_emb_512_masked[idx_tensor].mean(dim=0).tolist()\n",
    "    except:\n",
    "        avg_emb_2048_eos = torch.zeros((4096,)).tolist()\n",
    "        avg_emb_2048_masked = torch.zeros((4096,)).tolist()\n",
    "        avg_emb_512_eos = torch.zeros((4096,)).tolist()\n",
    "        avg_emb_512_masked = torch.zeros((4096,)).tolist()\n",
    "        pass\n",
    "\n",
    "    cat_emb_lv3_2048_eos.append(avg_emb_2048_eos)\n",
    "    cat_emb_lv3_2048_masked.append(avg_emb_2048_masked)\n",
    "    cat_emb_lv3_512_eos.append(avg_emb_512_eos)\n",
    "    cat_emb_lv3_512_masked.append(avg_emb_512_masked)\n",
    "\n",
    "cat_emb_lv3_2048_eos = torch.tensor(cat_emb_lv3_2048_eos).to(dtype=torch.float16)\n",
    "cat_emb_lv3_2048_masked = torch.tensor(cat_emb_lv3_2048_masked).to(dtype=torch.float16)\n",
    "cat_emb_lv3_512_eos = torch.tensor(cat_emb_lv3_512_eos).to(dtype=torch.float16)\n",
    "cat_emb_lv3_512_masked = torch.tensor(cat_emb_lv3_512_masked).to(dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b22c004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1891, 1891, 1891, 1891, 1891)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lv3_idx2key.keys()), len(cat_emb_lv3_2048_eos), len(cat_emb_lv3_2048_masked), len(cat_emb_lv3_512_eos), len(cat_emb_lv3_512_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e89a39ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_lv3_emb_list = [cat_emb_lv3_2048_masked, cat_emb_lv3_512_masked, cat_emb_lv3_2048_eos, cat_emb_lv3_512_eos]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671ed2ab-9a17-49b2-bfc2-f6c3361febe6",
   "metadata": {},
   "source": [
    "### Lv 5 Embedding 을 상위 (Lv 4) 카테고리별로 군집한 후 평균 계산\n",
    "---\n",
    "- 상위 레벨 (Lv4)에 공통으로 속하는 Lv 5 embedding 을 평균낸 임베딩을 해당 상위 레벨을 표현하는 임베딩으로 사용.\n",
    "- 상위 레벨 (Lv4)에 공통으로 속하는 Lv 5 embedding 을 tensor 로 묶어서 따로 저장."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5c53556",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_emb_lv4_2048_eos = []\n",
    "cat_emb_lv4_2048_masked = []\n",
    "cat_emb_lv4_512_eos = []\n",
    "cat_emb_lv4_512_masked = []\n",
    "for i, (k, vs) in enumerate(lv4_idx2key.items()):\n",
    "    try:\n",
    "        idx_tensor = torch.tensor(lv4_idx2lv5_idx_list[i])\n",
    "        avg_emb_2048_eos = cat_emb_2048_eos[idx_tensor].mean(dim=0).tolist()\n",
    "        avg_emb_2048_masked = cat_emb_2048_masked[idx_tensor].mean(dim=0).tolist()\n",
    "        avg_emb_512_eos = cat_emb_512_eos[idx_tensor].mean(dim=0).tolist()\n",
    "        avg_emb_512_masked = cat_emb_512_masked[idx_tensor].mean(dim=0).tolist()\n",
    "    except:\n",
    "        avg_emb_2048_eos = torch.zeros((4096,)).tolist()\n",
    "        avg_emb_2048_masked = torch.zeros((4096,)).tolist()\n",
    "        avg_emb_512_eos = torch.zeros((4096,)).tolist()\n",
    "        avg_emb_512_masked = torch.zeros((4096,)).tolist()\n",
    "        pass\n",
    "\n",
    "    cat_emb_lv4_2048_eos.append(avg_emb_2048_eos)\n",
    "    cat_emb_lv4_2048_masked.append(avg_emb_2048_masked)\n",
    "    cat_emb_lv4_512_eos.append(avg_emb_512_eos)\n",
    "    cat_emb_lv4_512_masked.append(avg_emb_512_masked)\n",
    "\n",
    "cat_emb_lv4_2048_eos = torch.tensor(cat_emb_lv4_2048_eos).to(dtype=torch.float16)\n",
    "cat_emb_lv4_2048_masked = torch.tensor(cat_emb_lv4_2048_masked).to(dtype=torch.float16)\n",
    "cat_emb_lv4_512_eos = torch.tensor(cat_emb_lv4_512_eos).to(dtype=torch.float16)\n",
    "cat_emb_lv4_512_masked = torch.tensor(cat_emb_lv4_512_masked).to(dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb789f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10792, 10792, 10792, 10792, 10792)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lv4_idx2key.keys()), len(cat_emb_lv4_2048_eos), len(cat_emb_lv4_2048_masked), len(cat_emb_lv4_512_eos), len(cat_emb_lv4_512_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4bf5d3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_lv4_emb_list = [cat_emb_lv4_2048_masked, cat_emb_lv4_512_masked, cat_emb_lv4_2048_eos, cat_emb_lv4_512_eos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0685d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "lv1idx2lv2idx = defaultdict(set)\n",
    "for k, vs in lv1idx2lv5idx.items():\n",
    "    for v in vs:\n",
    "        lv1idx2lv2idx[k].add(lv5_idx2lv2_idx[str(v)])\n",
    "        \n",
    "lv1idx2lv2idx = {k:list(v) for k, v in lv1idx2lv2idx.items()}\n",
    "\n",
    "key_dump, val_dump = [], []\n",
    "for k, v in lv1idx2lv2idx.items():\n",
    "    key_dump.append(k)\n",
    "    val_dump.extend(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2277c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lv1idx2lv3idx = defaultdict(set)\n",
    "for k, vs in lv1idx2lv5idx.items():\n",
    "    for v in vs:\n",
    "        lv1idx2lv3idx[k].add(lv5_idx2lv3_idx[str(v)])\n",
    "        \n",
    "lv1idx2lv3idx = {k:list(v) for k, v in lv1idx2lv3idx.items()}\n",
    "\n",
    "key_dump, val_dump = [], []\n",
    "for k, v in lv1idx2lv3idx.items():\n",
    "    key_dump.append(k)\n",
    "    val_dump.extend(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5cd66e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lv1idx2lv4idx = defaultdict(set)\n",
    "for k, vs in lv1idx2lv5idx.items():\n",
    "    for v in vs:\n",
    "        lv1idx2lv4idx[k].add(lv5_idx2lv4_idx[str(v)])\n",
    "        \n",
    "lv1idx2lv4idx = {k:list(v) for k, v in lv1idx2lv4idx.items()}\n",
    "\n",
    "key_dump, val_dump = [], []\n",
    "for k, v in lv1idx2lv4idx.items():\n",
    "    key_dump.append(k)\n",
    "    val_dump.extend(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4c6bdb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "lv2idx2lv1idx, lv3idx2lv1idx, lv4idx2lv1idx = {}, {}, {}\n",
    "for k, vs in lv1idx2lv2idx.items():\n",
    "    for v in vs:\n",
    "        lv2idx2lv1idx[v]=int(k)\n",
    "        \n",
    "for k, vs in lv1idx2lv3idx.items():\n",
    "    for v in vs:\n",
    "        lv3idx2lv1idx[v]=int(k)\n",
    "        \n",
    "for k, vs in lv1idx2lv4idx.items():\n",
    "    for v in vs:\n",
    "        lv4idx2lv1idx[v]=int(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d12db6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_mixup_lv1(avg_cat, emb_lv1, lam):\n",
    "    mixed_emb = (1-lam)*avg_cat + lam*emb_lv1\n",
    "    assert mixed_emb.shape == avg_cat.shape\n",
    "    return mixed_emb\n",
    "\n",
    "def emb_mixup_lv234(avg_cat, emb_lv1, lam, lvidx2lv1idx):\n",
    "    mixed_emb = []\n",
    "    for i in range(len(avg_cat)):\n",
    "        try:\n",
    "            emb = (1-lam)*avg_cat[i] + lam*emb_lv1[lvidx2lv1idx[i]]\n",
    "            mixed_emb.append(emb.tolist())\n",
    "        except:\n",
    "            mixed_emb.append(avg_cat[i].tolist())\n",
    "            pass\n",
    "        \n",
    "    mixed_emb = torch.tensor(mixed_emb)\n",
    "    assert mixed_emb.shape == avg_cat.shape\n",
    "    return mixed_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a0e937b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_method = [\"masked\", \"masked\", \"eos\", \"eos\"]\n",
    "lenght = [2048, 512, 2048, 512]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f260ce1-7910-4183-bb89-ab0c62a3e82a",
   "metadata": {},
   "source": [
    "### 각 상위 레벨 (Lv 2, 3, 4) 평균 embedding과 국문 조달청 Lv 1 Embedding Mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "62bfc6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lv = 2\n",
    "for i in range(4):\n",
    "    for l in [0.1, 0.5]:\n",
    "        mixed_emb_name = f\"mix_w_zod_{int(l*100)}_emb_lv_{lv}_max_leng_{lenght[i]}_{emb_method[i]}\"\n",
    "        mixed_embs = emb_mixup_lv234(avg_lv2_emb_list[i], zod_emb_list[i], l, lv2idx2lv1idx)\n",
    "        mixed_embs = mixed_embs.to(dtype=torch.float16)\n",
    "        torch.save(mixed_embs, f'../../../kisti_output/{mixed_emb_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "40141308",
   "metadata": {},
   "outputs": [],
   "source": [
    "lv = 3\n",
    "for i in range(4):\n",
    "    for l in [0.1, 0.5]:\n",
    "        mixed_emb_name = f\"mix_w_zod_{int(l*100)}_emb_lv_{lv}_max_leng_{lenght[i]}_{emb_method[i]}\"\n",
    "        mixed_embs = emb_mixup_lv234(avg_lv3_emb_list[i], zod_emb_list[i], l, lv3idx2lv1idx)\n",
    "        mixed_embs = mixed_embs.to(dtype=torch.float16)\n",
    "        torch.save(mixed_embs, f'../../../kisti_output/{mixed_emb_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d3e6f202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "lv = 4\n",
    "for i in range(4):\n",
    "    for l in [0.1, 0.5]:\n",
    "        mixed_emb_name = f\"mix_w_zod_{int(l*100)}_emb_lv_{lv}_max_leng_{lenght[i]}_{emb_method[i]}\"\n",
    "        mixed_embs = emb_mixup_lv234(avg_lv4_emb_list[i], zod_emb_list[i], l, lv4idx2lv1idx)\n",
    "        # print(mixed_embs.dtype)\n",
    "        mixed_embs = mixed_embs.to(dtype=torch.float16)\n",
    "        torch.save(mixed_embs, f'../../../kisti_output/{mixed_emb_name}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0929b682-9442-4f9c-b5fb-80bd164012bd",
   "metadata": {},
   "source": [
    "### 각 상위 레벨 (Lv 2, 3, 4) 평균 embedding과 UNSPSC Lv 1 Embedding Mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f2761428",
   "metadata": {},
   "outputs": [],
   "source": [
    "lv = 2\n",
    "for i in range(4):\n",
    "    for l in [0.1, 0.5]:\n",
    "        mixed_emb_name = f\"mix_w_uns_{int(l*100)}_emb_lv_{lv}_max_leng_{lenght[i]}_{emb_method[i]}\"\n",
    "        mixed_embs = emb_mixup_lv234(avg_lv2_emb_list[i], uni_emb_list[i], l, lv2idx2lv1idx)\n",
    "        mixed_embs = mixed_embs.to(dtype=torch.float16)\n",
    "        torch.save(mixed_embs, f'../../../kisti_output/{mixed_emb_name}.pt')\n",
    "        \n",
    "lv = 3\n",
    "for i in range(4):\n",
    "    for l in [0.1, 0.5]:\n",
    "        mixed_emb_name = f\"mix_w_uns_{int(l*100)}_emb_lv_{lv}_max_leng_{lenght[i]}_{emb_method[i]}\"\n",
    "        mixed_embs = emb_mixup_lv234(avg_lv3_emb_list[i], uni_emb_list[i], l, lv3idx2lv1idx)\n",
    "        mixed_embs = mixed_embs.to(dtype=torch.float16)\n",
    "        torch.save(mixed_embs, f'../../../kisti_output/{mixed_emb_name}.pt')\n",
    "        \n",
    "lv = 4\n",
    "for i in range(4):\n",
    "    for l in [0.1, 0.5]:\n",
    "        mixed_emb_name = f\"mix_w_uns_{int(l*100)}_emb_lv_{lv}_max_leng_{lenght[i]}_{emb_method[i]}\"\n",
    "        mixed_embs = emb_mixup_lv234(avg_lv4_emb_list[i], uni_emb_list[i], l, lv4idx2lv1idx)\n",
    "        mixed_embs = mixed_embs.to(dtype=torch.float16)\n",
    "        torch.save(mixed_embs, f'../../../kisti_output/{mixed_emb_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2efef3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cat_emb_lv2_2048_eos, '../../../kisti_output/avg_emb_zodal_cat_lv_2_max_leng_2048_eos.pt')\n",
    "torch.save(cat_emb_lv2_2048_masked, '../../../kisti_output/avg_emb_zodal_cat_lv_2_max_leng_2048_masked.pt')\n",
    "torch.save(cat_emb_lv2_512_eos, '../../../kisti_output/avg_emb_zodal_cat_lv_2_max_leng_512_eos.pt')\n",
    "torch.save(cat_emb_lv2_512_masked, '../../../kisti_output/avg_emb_zodal_cat_lv_2_max_leng_512_masked.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "187486a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cat_emb_lv3_2048_eos, '../../../kisti_output/avg_emb_zodal_cat_lv_3_max_leng_2048_eos.pt')\n",
    "torch.save(cat_emb_lv3_2048_masked, '../../../kisti_output/avg_emb_zodal_cat_lv_3_max_leng_2048_masked.pt')\n",
    "torch.save(cat_emb_lv3_512_eos, '../../../kisti_output/avg_emb_zodal_cat_lv_3_max_leng_512_eos.pt')\n",
    "torch.save(cat_emb_lv3_512_masked, '../../../kisti_output/avg_emb_zodal_cat_lv_3_max_leng_512_masked.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3baa815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cat_emb_lv4_2048_eos, '../../../kisti_output/avg_emb_zodal_cat_lv_4_max_leng_2048_eos.pt')\n",
    "torch.save(cat_emb_lv4_2048_masked, '../../../kisti_output/avg_emb_zodal_cat_lv_4_max_leng_2048_masked.pt')\n",
    "torch.save(cat_emb_lv4_512_eos, '../../../kisti_output/avg_emb_zodal_cat_lv_4_max_leng_512_eos.pt')\n",
    "torch.save(cat_emb_lv4_512_masked, '../../../kisti_output/avg_emb_zodal_cat_lv_4_max_leng_512_masked.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "86e0eb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./lv1idx2lv2idx.json\", \"w\") as f:\n",
    "    json.dump(lv1idx2lv2idx, f)\n",
    "    \n",
    "with open(\"./lv1idx2lv3idx.json\", \"w\") as f:\n",
    "    json.dump(lv1idx2lv2idx, f)\n",
    "\n",
    "with open(\"./lv1idx2lv4idx.json\", \"w\") as f:\n",
    "    json.dump(lv1idx2lv2idx, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "llama2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
