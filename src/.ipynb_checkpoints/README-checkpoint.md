# LLaMA Efficient Tuning
ì°¸ê³  REPO https://github.com/hiyouga/LLaMA-Efficient-Tuning

# Huggingface ëª¨ë¸ì„ ê°€ì ¸ì˜¤ëŠ” weigth ì‹ ì²­
 - ì‹ ì²­ ì£¼ì†Œ : https://ai.meta.com/resources/models-and-libraries/llama-downloads/

# ìµœì¢… í•™ìŠµëœ Weight
  - checkpoint-38400

## Provided Datasets
- KISTI Fine-tuning:
  - data/kisti_train.json

# ì‹¤í—˜ì„ ìœ„í•œ ê°€ìƒí™˜ê²½ ì„¤ì¹˜
```bash
git clone https://github.com/hiyouga/LLaMA-Efficient-Tuning.git
cd LLaMA-Efficient-Tuning
conda create -n llama_etuning python=3.10
conda activate llama_etuning
pip install -r requirements.txt
```

## Requirement

- Python 3.8+ and PyTorch 1.13.1+
- ğŸ¤—Transformers, Datasets, Accelerate, PEFT and TRL
- sentencepiece and tiktoken
- jieba, rouge-chinese and nltk (used at evaluation)
- gradio and matplotlib (used in web_demo.py)
- uvicorn, fastapi and sse-starlette (used in api_demo.py)

## í•™ìŠµì „ dataset ì¤€ë¹„

- í•™ìŠµì„ ìœ„í•œ KISTI ë°ì´í„° ì˜®ê¸°ê¸°

  - /preparing_dataset/fine_tuning_data ì—ì„œ kisti_train.json ê³¼ kisti_test.json ì„ /src/data/ ë¡œ ë³µì‚¬

- Validation data ìƒì„±í•˜ê¸°.

  - í•™ìŠµ ë°ì´í„° (kisti_train.json) ë¥¼ task ë§ˆë‹¤ ë™ì¼í•œ ë¹„ìœ¨ë¡œ train ê³¼ validation dataë¡œ êµ¬ë¶„. 

    ```bash
    python data_split.py
    ```

## ì‚¬ìš©í•  Dataset ë“±ë¡

- LLaMA-Efficient-Tuning/data ì— data ì˜®ê¸°ê¸°
- LLaMA-Efficient-Tuning/data/data_info.json íŒŒì¼ì— ì•„ë˜ì²˜ëŸ¼ ë°ì´í„° ë“±ë¡

```bash
  "dataset_name": {
    "file_name": "data_file_name.json" },
 *shardëŠ” ì§€ì • ì•ˆ í•´ë„ë¨
```

## ì—¬ëŸ¬ê°œì˜ GPUë¥¼ ì´ìš©í•˜ì—¬ í•™ìŠµ
- Deepspeed ì´ìš©
- ì—¬ëŸ¬ê°œì˜ gpuë¥¼ ì‚¬ìš©í•  ê²½ìš° deepspeedë¥¼ ì´ìš©í• ìˆ˜ ìˆë‹¤.
- ì‚¬ìš©í•œ deepspeed config íŒŒì¼ í•´ë‹¹ configëŠ” ds_deepspeed.jsonì— ì €ì¥

```bash
  pip install deepspeed
```


```json
  {
      "train_micro_batch_size_per_gpu": "auto",
      "gradient_accumulation_steps": "auto",
      "gradient_clipping": "auto",
      "zero_allow_untested_optimizer": true,
      "fp16": {
        "enabled": "auto",
        "loss_scale": 1,
        "initial_scale_power": 16,
        "loss_scale_window": 1000,
        "hysteresis": 2,
        "min_loss_scale": 1
      },  
      "zero_optimization": {
        "stage": 2,
        "allgather_partitions": true,
        "allgather_bucket_size": 5e8,
        "reduce_scatter": true,
        "reduce_bucket_size": 5e8,
        "overlap_comm": false,
        "contiguous_gradients": true
      }
    }
```

### í•™ìŠµ ì»¤ë§¨ë“œ
```bash
deepspeed â€“-num_gpus 8 

train/train_bash.py
   --local_rank=0 \
   --deepspeed ds_deepspeed.json \
   --stage sft \ #supervised tuning
   --model_name_or_path Llama-2-7b-chat-hf \ # huggingface ëª¨ë¸ weight ê²½ë¡œ 
   --do_train \ #  í•™ìŠµì„ í•œë‹¤ëŠ” command
   --dataset kisti_data \ # í•™ìŠµí•  ë°ì´í„°ì…‹ ì§€ì •
   --template llama2 \ # templateì„ ì§€ì •í• ìˆ˜ ìˆìŒ, llama2ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ llama2ì˜ template ì‚¬ìš©
   --finetuning_type lora \ # ë¹ ë¥¸ í•™ìŠµì„ ìœ„í•œ lora ë°©ì‹ ì‚¬ìš©
   --lora_target q_proj,v_proj \
   --output_dir /path/to/save \ # ìµœì¢… ëª¨ë¸ì„ ì €ì¥í•  ê²½ë¡œë¥¼ ì§€ì •
   --per_device_train_batch_size 2 \ # train datasetì˜ batchsize ì§€ì •(ì…ë ¥ê¸¸ì´ë¥¼ ê¸¸ê²Œ ì„¤ì •í–ˆê¸°ë•Œë¬¸ì— batch í¬ê¸°ê°€ ì‘ìŒ)
   --gradient_accumulation_steps 4 \
   --lr_scheduler_type cosine \
   --logging_steps 1000 \
   --save_steps 1920 \
   --learning_rate 1e-6 \
   --plot_loss \
   --bf16 \ # ë¹ ë¥¸ í•™ìŠµì„ ìœ„í•œ precision ì„¤ì •
   --val_size 0.1 \
   --do_eval \ # ê²€ì¦ ë‹¨ê³„ê°€ í•„ìš”í• ì‹œ ì¶”ê°€
   --evaluation_strategy steps \
   --eval_steps 2000 \
   --num_train_epochs 10 \ #í•™ìŠµ ì—í­ 
   --max_source_length 2048 \ # ì…ë ¥ê¸¸ì´
   --max_target_length 2048 \ # ì¶œë ¥ê¸¸ì´
   --overwrite_cache \ # cache overwrtie ìœ ë¬´
   --checkpoint_dir /path/checkpoints # tuningí•  ëª¨ë¸ì˜ weight
```

### ìƒì„± ì»¤ë§¨ë“œ

```bash
CUDA_VISIBLE_DEVICES=0 
    python train/train_bash.py \
    --stage sft \
    --model_name_or_path path_to_llama_model \
    --do_predict \
    --dataset kisti_test \
    --template default \
    --finetuning_type lora \
    --checkpoint_dir path_to_checkpoint \
    --output_dir path_to_predict_result \
    --per_device_eval_batch_size 8 \
    --max_samples 100 \
    --max_source_length 2048 \
    --max_target_length 2048 \
    --predict_with_generate
```
## Embedding mapping (ìƒí’ˆ-ì¹´í…Œê³ ë¦¬ ë§µí•‘)
- train/extract_emb.py : ìƒí’ˆ, ì¹´í…Œê³ ë¦¬ ì„ë² ë”© ì¶”ì¶œ ì½”ë“œ
- train/task_12.py : task 1,2 (Amazon ìƒí’ˆ - ì•„ë§ˆì¡´ ì¹´í…Œê³ ë¦¬ ë§µí•‘, ë‹¤ë‚˜ì™€ ìƒí’ˆ - ë‹¤ë‚˜ì™€ ì¹´í…Œê³ ë¦¬ ë§µí•‘)
- train/eval_tasks.py : task 3 (Amazon, ë‹¨ë‚˜ì™€ 50ê°œ ìƒí’ˆ - ì „ììƒê±°ë˜ í‘œì¤€ë¶„ë¥˜ì²´ê³„ ë§µí•‘)

### ì„ë² ë”© ìƒì„±
- exp_bash/get_embdding.sh

### Task 1, 2, 3 ìˆ˜í–‰
- exp_bash/eval_task12.sh
- exp_bash/eval_task3.sh

### Task 4 ìˆ˜í–‰
ìƒì„± ì»¤ë§¨ë“œì™€ ë™ì¼í•˜ë‚˜ task generatingí´ë”ì˜ ì‹¤í–‰íŒŒì¼ì— ì˜í•´ ìƒì„±ëœ taskXX.jsoníŒŒì¼ì„ inputê°’ìœ¼ë¡œ í•˜ì—¬ ìƒì„±
í”„ë¡¬í”„íŠ¸ ë³€ê²½ì„ ì›í•  ê²½ìš°, task4_generate.ipynbì—ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ êµì²´í•˜ë©´ë¨.
```bash
CUDA_VISIBLE_DEVICES=0
    python train/train_bash.py \
    --stage sft \
    --model_name_or_path path_to_llama_model \
    --do_predict \
    --dataset kistitaskXX \ #kistitaskXX.json task4 ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ìƒì„±ì„ ì§„í–‰ #ë°ì´í„°ì…‹ ë“±ë¡í•´ì•¼í•¨
    --template default \
    --finetuning_type lora \
    --checkpoint_dir path_to_checkpoint \
    --output_dir path_to_predict_result \
    --per_device_eval_batch_size 8 \
    --max_source_length 2048 \
    --max_target_length 2048 \
    --predict_with_generate
```